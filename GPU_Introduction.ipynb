{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to GPU computing\n",
    "GPUs can calculate faster than CPUs for three architectural reasons\n",
    "1. Many-core architecture allows for massive parallelism. \n",
    "   Each core does a very similar operation on a few elements a large dataset.\n",
    "2. High memory bandwidth helps move to and from the GPU compute cores.\n",
    "3. Oversubscription (more threads than cores) hides memory-transfer latency.\n",
    "\n",
    "## First, look at performance on the CPU\n",
    "\n",
    "We create an array with a million elements and do a silly operation on it to \n",
    "create some busy work. The operation involves square roots and sine functions\n",
    "to create \n",
    "The function we choose is this silly thing:\n",
    "$$\n",
    "f(x) = \\sqrt{x\\mod 13} + \\sum_{n=50}^{53} \\sin(x\\mod n)\n",
    "$$\n",
    "(The mod function returns the division remainder.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numba import jit, vectorize\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 10000000\n",
    "a = np.linspace(0, N, N) # 0, 1, 2, ..., N\n",
    "b = np.empty_like(a) # Allocated but uninitialized memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def silly_operation(a, b):\n",
    "    \"\"\"\n",
    "    Some mathematical busywork to illustrate GPU vs CPU\n",
    "    \"\"\"\n",
    "    for i in range(len(a)):\n",
    "        b[i] = math.sqrt(a[i] % 13)\n",
    "        for j in range(50, 54):\n",
    "            b[i] += math.sin(a[i] % j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = timer()\n",
    "silly_operation(a,b)\n",
    "end = timer()\n",
    "print(\"function took %g s\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should note that python is naturally single core, so we're missing a factor of\n",
    "12x to 24x if we could exploit all CPU cores.\n",
    "\n",
    "## Vectorized CPU code\n",
    "\n",
    "One performance improvement is to note that this function operates\n",
    "on each element individually, so it can be vectorized.\n",
    "This helps performance some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@vectorize('float64(float64)')\n",
    "def vectorized_silly_operation(a):\n",
    "    \"\"\"\n",
    "    Some mathematical busywork to illustrate GPU vs CPU\n",
    "    \"\"\"\n",
    "    b = math.sqrt(a % 13)\n",
    "    for j in range(50, 54):\n",
    "        b += math.sin(a % (j))\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = timer()\n",
    "b = vectorized_silly_operation(a)\n",
    "end = timer()\n",
    "print(\"function took %g s\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Acceleration using CUDA\n",
    "\n",
    "We can run this faster using CPU hardware.\n",
    "\n",
    "The first step is to set up the arrays in GPU device memory.\n",
    "The input array needs to be copied over, and we need to allocate space for \n",
    "the output array on the GPU device. \n",
    "After the GPU operation, we will need to copy the output back to host memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_a = cuda.to_device(a) # Copy the input data to GPU (device) memory\n",
    "dev_b = cuda.device_array_like(a) # Allocate but do not initialize output in GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_a.alloc_size, dev_b.alloc_size # Ask how many bytes are allocated on the GPU device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we write a CUDA kernel. This looks a lot like the vectorized function.\n",
    "CUDA will set this up with blocks of threads.\n",
    "In C++/CUDA, we'd need to set up the blocks and threads and do a bit of bookkeeping.\n",
    "In numba cuda we can simply use the cuda.grid(1) to automatically turn the block\n",
    "and thread ids into an index i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def cu_silly_operation(a, b):\n",
    "    \"\"\"\n",
    "    Some mathematical busywork to illustrate GPU vs CPU\n",
    "    \"\"\"\n",
    "    i = cuda.grid(1)\n",
    "    b[i] = math.sqrt(a[i] % 13)\n",
    "    for n in range(4):\n",
    "        b[i] += math.sin(a[i] % (n + 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba CUDA also allows us to call the kernel without explicity supplying the thread and block\n",
    "size. While we can do this on our own, it's convenient to let numba sort this out from the array dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = timer()\n",
    "#cu_silly_operation[N//32, 32](dev_a, dev_b)\n",
    "cu_silly_operation(dev_a, dev_b)\n",
    "end = timer()\n",
    "print(\"function took %g s\" % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the calcuation, use the copy_to_host method to copy back to the CPU array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_b.copy_to_host(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
